{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Spark ClickHouse Connector is a high performance connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol. Requirements Basic knowledge of Apache Spark and ClickHouse . An available ClickHouse single node or cluster, and ClickHouse version should at least v21.1.2.15-stable , because Spark communicates with ClickHouse through the gRPC protocol. An available Spark cluster, and Spark version should be 3.2 or 3.3, because we need the interfaces of Spark DataSource V2 added in 3.2.0 or 3.3.0. Make sure your network policy satisfies the following requirements, both driver and executor of Spark need to access ClickHouse gRPC port. If you are using it to access ClickHouse cluster, ensure the connectivity between driver and executor of Spark and each node of ClickHouse cluster. Notes Integration tests based on Java 8&11, Scala 2.12&2.13, Spark 3.2.1&3.3.0 and ClickHouse v22.3.3.44-lts, with both single ClickHouse instance and ClickHouse cluster.","title":"Home"},{"location":"#overview","text":"Spark ClickHouse Connector is a high performance connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol.","title":"Overview"},{"location":"#requirements","text":"Basic knowledge of Apache Spark and ClickHouse . An available ClickHouse single node or cluster, and ClickHouse version should at least v21.1.2.15-stable , because Spark communicates with ClickHouse through the gRPC protocol. An available Spark cluster, and Spark version should be 3.2 or 3.3, because we need the interfaces of Spark DataSource V2 added in 3.2.0 or 3.3.0. Make sure your network policy satisfies the following requirements, both driver and executor of Spark need to access ClickHouse gRPC port. If you are using it to access ClickHouse cluster, ensure the connectivity between driver and executor of Spark and each node of ClickHouse cluster.","title":"Requirements"},{"location":"#notes","text":"Integration tests based on Java 8&11, Scala 2.12&2.13, Spark 3.2.1&3.3.0 and ClickHouse v22.3.3.44-lts, with both single ClickHouse instance and ClickHouse cluster.","title":"Notes"},{"location":"best_practices/","text":"TODO","title":"Index"},{"location":"best_practices/#todo","text":"","title":"TODO"},{"location":"best_practices/01_deployment/","text":"Deployment Jar Put clickhouse-spark-runtime-3.3_2.12-{version}.jar into $SPARK_HOME/jars/ , then you don't need to bundle the jar into your Spark application, and --jar is not required when using spark-shell or spark-sql (again, for SQL-only use cases, Apache Kyuubi(Incubating) is recommended for Production). Configuration Persist catalog configurations into $SPARK_HOME/conf/spark-defaults.conf , then --conf s are not required when using spark-shell or spark-sql . spark.sql.catalog.ck_01=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_01.host=10.0.0.1 spark.sql.catalog.ck_01.grpc_port=9100 spark.sql.catalog.ck_01.user=app spark.sql.catalog.ck_01.password=pwd spark.sql.catalog.ck_01.database=default spark.sql.catalog.ck_02=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_02.host=10.0.0.2 spark.sql.catalog.ck_02.grpc_port=9100 spark.sql.catalog.ck_02.user=app spark.sql.catalog.ck_02.password=pwd spark.sql.catalog.ck_02.database=default","title":"Deployment"},{"location":"best_practices/01_deployment/#deployment","text":"","title":"Deployment"},{"location":"best_practices/01_deployment/#jar","text":"Put clickhouse-spark-runtime-3.3_2.12-{version}.jar into $SPARK_HOME/jars/ , then you don't need to bundle the jar into your Spark application, and --jar is not required when using spark-shell or spark-sql (again, for SQL-only use cases, Apache Kyuubi(Incubating) is recommended for Production).","title":"Jar"},{"location":"best_practices/01_deployment/#configuration","text":"Persist catalog configurations into $SPARK_HOME/conf/spark-defaults.conf , then --conf s are not required when using spark-shell or spark-sql . spark.sql.catalog.ck_01=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_01.host=10.0.0.1 spark.sql.catalog.ck_01.grpc_port=9100 spark.sql.catalog.ck_01.user=app spark.sql.catalog.ck_01.password=pwd spark.sql.catalog.ck_01.database=default spark.sql.catalog.ck_02=xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.ck_02.host=10.0.0.2 spark.sql.catalog.ck_02.grpc_port=9100 spark.sql.catalog.ck_02.user=app spark.sql.catalog.ck_02.password=pwd spark.sql.catalog.ck_02.database=default","title":"Configuration"},{"location":"configurations/","text":"Configurations TODO Overwrite SQL Configurations Your can overwrite ClickHouse SQL Configurations by editing $SPARK_HOME/conf/spark-defaults.conf , e.g. spark.clickhouse.write.batchSize 10000 spark.clickhouse.write.maxRetry 2","title":"Index"},{"location":"configurations/#configurations","text":"","title":"Configurations"},{"location":"configurations/#todo","text":"","title":"TODO"},{"location":"configurations/#overwrite-sql-configurations","text":"Your can overwrite ClickHouse SQL Configurations by editing $SPARK_HOME/conf/spark-defaults.conf , e.g. spark.clickhouse.write.batchSize 10000 spark.clickhouse.write.maxRetry 2","title":"Overwrite SQL Configurations"},{"location":"configurations/01_catalog_configurations/","text":"Catalog Configurations Single Instance Suppose you have one ClickHouse instance which installed on 10.0.0.1 and expose gRPC at port 9100 . Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.grpc_port 9100 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> . Cluster For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and expose gRPC at port 9100 named clickhouse1, and another installed on 10.0.0.2 and expose gRPC at port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> .","title":"Catalog Configurations"},{"location":"configurations/01_catalog_configurations/#catalog-configurations","text":"","title":"Catalog Configurations"},{"location":"configurations/01_catalog_configurations/#single-instance","text":"Suppose you have one ClickHouse instance which installed on 10.0.0.1 and expose gRPC at port 9100 . Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse.host 10.0.0.1 spark.sql.catalog.clickhouse.grpc_port 9100 spark.sql.catalog.clickhouse.user default spark.sql.catalog.clickhouse.password spark.sql.catalog.clickhouse.database default Then you can access ClickHouse table <ck_db>.<ck_table> from Spark SQL by using clickhouse.<ck_db>.<ck_table> .","title":"Single Instance"},{"location":"configurations/01_catalog_configurations/#cluster","text":"For ClickHouse cluster, give an unique catalog name for each instances. Suppose you have two ClickHouse instances, one installed on 10.0.0.1 and expose gRPC at port 9100 named clickhouse1, and another installed on 10.0.0.2 and expose gRPC at port 9100 named clickhouse2. Edit $SPARK_HOME/conf/spark-defaults.conf . spark.sql.catalog.clickhouse1 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse1.host 10.0.0.1 spark.sql.catalog.clickhouse1.grpc_port 9100 spark.sql.catalog.clickhouse1.user default spark.sql.catalog.clickhouse1.password spark.sql.catalog.clickhouse1.database default spark.sql.catalog.clickhouse2 xenon.clickhouse.ClickHouseCatalog spark.sql.catalog.clickhouse2.host 10.0.0.2 spark.sql.catalog.clickhouse2.grpc_port 9100 spark.sql.catalog.clickhouse2.user default spark.sql.catalog.clickhouse2.password spark.sql.catalog.clickhouse2.database default Then you can access clickhouse1 table <ck_db>.<ck_table> from Spark SQL by clickhouse1.<ck_db>.<ck_table> , and access clickhouse2 table <ck_db>.<ck_table> by clickhouse2.<ck_db>.<ck_table> .","title":"Cluster"},{"location":"configurations/02_sql_configurations/","text":"SQL Configurations Since 0.1.0 - spark.clickhouse.write.batchSize Default Value: 10000 Description: The number of records per batch on writing to ClickHouse. Since 0.1.0 - spark.clickhouse.write.maxRetry Default Value: 3 Description: The maximum number of write we will retry for a single batch write failed with retryable codes. Since 0.1.0 - spark.clickhouse.write.retryInterval Default Value: 10 Description: The interval in seconds between write retry. Since 0.1.0 - spark.clickhouse.write.retryableErrorCodes Default Value: 241 Description: The retryable error codes returned by ClickHouse server when write failing. Since 0.1.0 - spark.clickhouse.write.repartitionNum Default Value: 0 Description: Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. Since 0.3.0 - spark.clickhouse.write.repartitionByPartition Default Value: true Description: Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. Since 0.3.0 - spark.clickhouse.write.repartitionStrictly Default Value: false Description: If true, Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523, w/o this patch, it always act as true . Since 0.1.0 - spark.clickhouse.write.distributed.useClusterNodes Default Value: true Description: Write to all nodes of cluster when writing Distributed table. Since 0.1.0 - spark.clickhouse.read.distributed.useClusterNodes Default Value: false Description: Read from all nodes of cluster when reading Distributed table. Since 0.1.0 - spark.clickhouse.write.distributed.convertLocal Default Value: false Description: When writing Distributed table, write local table instead of itself. If true , ignore write.distributed.useClusterNodes . Since 0.1.0 - spark.clickhouse.read.distributed.convertLocal Default Value: true Description: When reading Distributed table, read local table instead of itself. If true , ignore read.distributed.useClusterNodes . Since 0.4.0 - spark.clickhouse.read.splitByPartitionId Default Value: true Description: If true , construct input partition filter by virtual column _partition_id , instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+. Since 0.3.0 - spark.clickhouse.write.localSortByPartition Default Value: spark.clickhouse.write.repartitionByPartition Description: If true , do local sort by partition before writing. Since 0.3.0 - spark.clickhouse.write.localSortByKey Default Value: true Description: If true , do local sort by sort keys before writing. Since 0.3.0 - spark.clickhouse.write.compression.codec Default Value: lz4 Description: The codec used to compress data for writing. Supported codecs: none, gzip, lz4, zstd. Since 0.4.0 - spark.clickhouse.write.compression.zstd.level Default Value: 3 Description: Zstd compression level. Since 0.4.0 - spark.clickhouse.write.compression.zstd.thread Default Value: 0 Description: Zstd worker pool size. Default value is 0 , aka 'single-threaded mode': no worker is spawned. Spawn worker threads and trigger asynchronous mode when value equals or greater than 1. Since 0.4.0 - spark.clickhouse.write.format Default Value: ArrowStream Description: Serialize format for writing. Supported formats: JSONEachRow, ArrowStream.","title":"SQL Configurations"},{"location":"configurations/02_sql_configurations/#sql-configurations","text":"Since 0.1.0 - spark.clickhouse.write.batchSize Default Value: 10000 Description: The number of records per batch on writing to ClickHouse. Since 0.1.0 - spark.clickhouse.write.maxRetry Default Value: 3 Description: The maximum number of write we will retry for a single batch write failed with retryable codes. Since 0.1.0 - spark.clickhouse.write.retryInterval Default Value: 10 Description: The interval in seconds between write retry. Since 0.1.0 - spark.clickhouse.write.retryableErrorCodes Default Value: 241 Description: The retryable error codes returned by ClickHouse server when write failing. Since 0.1.0 - spark.clickhouse.write.repartitionNum Default Value: 0 Description: Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. Since 0.3.0 - spark.clickhouse.write.repartitionByPartition Default Value: true Description: Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. Since 0.3.0 - spark.clickhouse.write.repartitionStrictly Default Value: false Description: If true, Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523, w/o this patch, it always act as true . Since 0.1.0 - spark.clickhouse.write.distributed.useClusterNodes Default Value: true Description: Write to all nodes of cluster when writing Distributed table. Since 0.1.0 - spark.clickhouse.read.distributed.useClusterNodes Default Value: false Description: Read from all nodes of cluster when reading Distributed table. Since 0.1.0 - spark.clickhouse.write.distributed.convertLocal Default Value: false Description: When writing Distributed table, write local table instead of itself. If true , ignore write.distributed.useClusterNodes . Since 0.1.0 - spark.clickhouse.read.distributed.convertLocal Default Value: true Description: When reading Distributed table, read local table instead of itself. If true , ignore read.distributed.useClusterNodes . Since 0.4.0 - spark.clickhouse.read.splitByPartitionId Default Value: true Description: If true , construct input partition filter by virtual column _partition_id , instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+. Since 0.3.0 - spark.clickhouse.write.localSortByPartition Default Value: spark.clickhouse.write.repartitionByPartition Description: If true , do local sort by partition before writing. Since 0.3.0 - spark.clickhouse.write.localSortByKey Default Value: true Description: If true , do local sort by sort keys before writing. Since 0.3.0 - spark.clickhouse.write.compression.codec Default Value: lz4 Description: The codec used to compress data for writing. Supported codecs: none, gzip, lz4, zstd. Since 0.4.0 - spark.clickhouse.write.compression.zstd.level Default Value: 3 Description: Zstd compression level. Since 0.4.0 - spark.clickhouse.write.compression.zstd.thread Default Value: 0 Description: Zstd worker pool size. Default value is 0 , aka 'single-threaded mode': no worker is spawned. Spawn worker threads and trigger asynchronous mode when value equals or greater than 1. Since 0.4.0 - spark.clickhouse.write.format Default Value: ArrowStream Description: Serialize format for writing. Supported formats: JSONEachRow, ArrowStream.","title":"SQL Configurations"},{"location":"developers/","text":"TODO","title":"Index"},{"location":"developers/#todo","text":"","title":"TODO"},{"location":"developers/01_build_and_test/","text":"Build and Test Build Check out source code from GitHub git checkout https://github.com/housepower/spark-clickhouse-connector.git Build w/o test ./gradlew clean build -x test Go to spark-3.3/clickhouse-spark-runtime/build/libs/ to find the output jar clickhouse-spark-runtime-3.3_2.12-${version}.jar . Test The project leverage Testcontainers and Docker Compose to do integration tests, you should install Docker and Docker Compose before running test, and check more details on Testcontainers document if you'd like to run test with remote Docker daemon. Run all test ./gradlew clean test Run single test ./gradlew test --tests=ConvertDistToLocalWriteSuite ARM Platform For developers/users who use ARM platform, e.g. Apple Silicon chips, Kunpeng chips, you may not run integrations test in local directly, because ClickHouse does not provide gRPC support in official ARM image . As a workaround, you can set the environment variable CLICKHOUSE_IMAGE to use a custom image which supports gRPC on ARM platform for testing. export CLICKHOUSE_IMAGE=pan3793/clickhouse-server:22.5.1-alpine-arm-grpc ./gradlew clean test","title":"Build and Test"},{"location":"developers/01_build_and_test/#build-and-test","text":"","title":"Build and Test"},{"location":"developers/01_build_and_test/#build","text":"Check out source code from GitHub git checkout https://github.com/housepower/spark-clickhouse-connector.git Build w/o test ./gradlew clean build -x test Go to spark-3.3/clickhouse-spark-runtime/build/libs/ to find the output jar clickhouse-spark-runtime-3.3_2.12-${version}.jar .","title":"Build"},{"location":"developers/01_build_and_test/#test","text":"The project leverage Testcontainers and Docker Compose to do integration tests, you should install Docker and Docker Compose before running test, and check more details on Testcontainers document if you'd like to run test with remote Docker daemon. Run all test ./gradlew clean test Run single test ./gradlew test --tests=ConvertDistToLocalWriteSuite","title":"Test"},{"location":"developers/01_build_and_test/#arm-platform","text":"For developers/users who use ARM platform, e.g. Apple Silicon chips, Kunpeng chips, you may not run integrations test in local directly, because ClickHouse does not provide gRPC support in official ARM image . As a workaround, you can set the environment variable CLICKHOUSE_IMAGE to use a custom image which supports gRPC on ARM platform for testing. export CLICKHOUSE_IMAGE=pan3793/clickhouse-server:22.5.1-alpine-arm-grpc ./gradlew clean test","title":"ARM Platform"},{"location":"developers/02_docs_and_website/","text":"Docs and Website Setup Python Follow the Python official document to install. Setup pyenv on macOS (optional) Optionally, recommend to manage Python environments by pyenv . Install from Homebrew brew install pyenv pyenv-virtualenv Setup in ~/.zshrc eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" Install virtualenv pyenv install 3.9.13 pyenv virtualenv 3.9.13 scc Localize virtualenv pyenv local scc Install dependencies pip install -r requirements.txt Preview website mkdocs serve Open http://127.0.0.1:8000/ in browser.","title":"Docs and Website"},{"location":"developers/02_docs_and_website/#docs-and-website","text":"","title":"Docs and Website"},{"location":"developers/02_docs_and_website/#setup-python","text":"Follow the Python official document to install.","title":"Setup Python"},{"location":"developers/02_docs_and_website/#setup-pyenv-on-macos-optional","text":"Optionally, recommend to manage Python environments by pyenv . Install from Homebrew brew install pyenv pyenv-virtualenv Setup in ~/.zshrc eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" Install virtualenv pyenv install 3.9.13 pyenv virtualenv 3.9.13 scc Localize virtualenv pyenv local scc","title":"Setup pyenv on macOS (optional)"},{"location":"developers/02_docs_and_website/#install-dependencies","text":"pip install -r requirements.txt","title":"Install dependencies"},{"location":"developers/02_docs_and_website/#preview-website","text":"mkdocs serve Open http://127.0.0.1:8000/ in browser.","title":"Preview website"},{"location":"developers/03_private_release/","text":"Private Release Tip Internal Release means deploying to private Nexus Repository. Please make sure you are granted to access your company private Nexus Repository. Repository and Authentication Configure Gradle in ~/.gradle/gradle.properties . mavenUser=xxx mavenPassword=xxx mavenReleasesRepo=xxx mavenSnapshotsRepo=xxx Upgrade Version Modify version in version.txt and docker/.env-dev Build and Deploy Publish to Maven Repository using ./gradlew publish","title":"Private Release"},{"location":"developers/03_private_release/#private-release","text":"Tip Internal Release means deploying to private Nexus Repository. Please make sure you are granted to access your company private Nexus Repository.","title":"Private Release"},{"location":"developers/03_private_release/#repository-and-authentication","text":"Configure Gradle in ~/.gradle/gradle.properties . mavenUser=xxx mavenPassword=xxx mavenReleasesRepo=xxx mavenSnapshotsRepo=xxx","title":"Repository and Authentication"},{"location":"developers/03_private_release/#upgrade-version","text":"Modify version in version.txt and docker/.env-dev","title":"Upgrade Version"},{"location":"developers/03_private_release/#build-and-deploy","text":"Publish to Maven Repository using ./gradlew publish","title":"Build and Deploy"},{"location":"developers/04_public_release/","text":"Public Release Notice Public Release means deploying to Maven Central. Only core team members are granted to deploy into Public Repository. Note Most of the steps for a public release are done by the GitHub workflow. Snapshot Release The daily snapshot release is managed by Publish Snapshot workflow, it is scheduled to be deployed at midnight every day. Feature Release Cut new branch from master branch, e.g. branch-0.3 ; Update version in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.3.0 ; Create new tag, e.g. v0.3.0 , it will trigger the Publish Release workflow; Verify, close, and release in Sonatype Repository Announce in GitHub Release Update version in version.txt and docker/.env-dev , e.g. from 0.3.0 to 0.3.1-SNAPSHOT ; Update version on master branch in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.4.0-SNAPSHOT ; Publish Docker image after jars available in Maven Central, generally it costs few minutes after step 3. Patch Release Just emit step 1 and step 7 from feature release.","title":"Public Release"},{"location":"developers/04_public_release/#public-release","text":"Notice Public Release means deploying to Maven Central. Only core team members are granted to deploy into Public Repository. Note Most of the steps for a public release are done by the GitHub workflow.","title":"Public Release"},{"location":"developers/04_public_release/#snapshot-release","text":"The daily snapshot release is managed by Publish Snapshot workflow, it is scheduled to be deployed at midnight every day.","title":"Snapshot Release"},{"location":"developers/04_public_release/#feature-release","text":"Cut new branch from master branch, e.g. branch-0.3 ; Update version in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.3.0 ; Create new tag, e.g. v0.3.0 , it will trigger the Publish Release workflow; Verify, close, and release in Sonatype Repository Announce in GitHub Release Update version in version.txt and docker/.env-dev , e.g. from 0.3.0 to 0.3.1-SNAPSHOT ; Update version on master branch in version.txt and docker/.env-dev , e.g. from 0.3.0-SNAPSHOT to 0.4.0-SNAPSHOT ; Publish Docker image after jars available in Maven Central, generally it costs few minutes after step 3.","title":"Feature Release"},{"location":"developers/04_public_release/#patch-release","text":"Just emit step 1 and step 7 from feature release.","title":"Patch Release"},{"location":"internals/","text":"Overview Design In high level, Spark ClickHouse Connector is a connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol.","title":"Index"},{"location":"internals/#overview-design","text":"In high level, Spark ClickHouse Connector is a connector build on top of Spark DataSource V2 and ClickHouse gRPC protocol.","title":"Overview Design"},{"location":"internals/01_catalog/","text":"Catalog Management One important end user facing feature of DataSource V2 is supporting of multi-catalogs. In the early stage of Spark, it does have catalog concept, usually, user use Hive Metastore or Glue to manage table metadata, hence user must register external DataSource tables in centralized metastore before using it. In the centralized metastore model, a table is identified by <database>.<table> . Things changed in DataSource V2, starting from v3.0.0, Spark introduced catalog concept, then a table is identified by <catalog>.<database>.<table> . The default catalog has a fixed name spark_catalog .","title":"Catalog"},{"location":"internals/01_catalog/#catalog-management","text":"One important end user facing feature of DataSource V2 is supporting of multi-catalogs. In the early stage of Spark, it does have catalog concept, usually, user use Hive Metastore or Glue to manage table metadata, hence user must register external DataSource tables in centralized metastore before using it. In the centralized metastore model, a table is identified by <database>.<table> . Things changed in DataSource V2, starting from v3.0.0, Spark introduced catalog concept, then a table is identified by <catalog>.<database>.<table> . The default catalog has a fixed name spark_catalog .","title":"Catalog Management"},{"location":"internals/02_read/","text":"How reading of the connector works? Push Down The connector implements most push down interfaces defined by DataSource V2, such as SupportsPushDownLimit , SupportsPushDownFilters , SupportsPushDownAggregates , SupportsPushDownRequiredColumns . The below example shows how SupportsPushDownAggregates and SupportsPushDownRequiredColumns work. No Push Down Aggregate Push Down Bucket Join Sort merge join is a general solution for two large table inner join, it requires two table shuffle by join key first, then do local sort by join key in each data partition, finally do stream-stream like look up to get the final result. Sort Merge Join Bucket Join","title":"Query Table"},{"location":"internals/02_read/#how-reading-of-the-connector-works","text":"","title":"How reading of the connector works?"},{"location":"internals/02_read/#push-down","text":"The connector implements most push down interfaces defined by DataSource V2, such as SupportsPushDownLimit , SupportsPushDownFilters , SupportsPushDownAggregates , SupportsPushDownRequiredColumns . The below example shows how SupportsPushDownAggregates and SupportsPushDownRequiredColumns work. No Push Down Aggregate Push Down","title":"Push Down"},{"location":"internals/02_read/#bucket-join","text":"Sort merge join is a general solution for two large table inner join, it requires two table shuffle by join key first, then do local sort by join key in each data partition, finally do stream-stream like look up to get the final result. Sort Merge Join Bucket Join","title":"Bucket Join"},{"location":"internals/03_write/","text":"How writing of the connector works?","title":"Write Data"},{"location":"internals/03_write/#how-writing-of-the-connector-works","text":"","title":"How writing of the connector works?"},{"location":"quick_start/01_get_the_library/","text":"Get the Library Download the Library The name pattern of binary jar is clickhouse-spark-runtime-${spark_binary_version}_${scala_binary_version}-${version}.jar , you can find all available released jars under Maven Central Repository and all daily build snapshot jars under Sonatype OSS Snapshots Repository . Tip The runtime jar contains and relocates all required classes, you do not need to worry about transitive dependencies management and potential class conflictions. Import as Dependency Gradle dependencies { implementation(\"com.github.housepower:clickhouse-spark-runtime-3.3_2.12:${version}\") } Add the following repository if you want to use SNAPSHOT version. repositries { maven { url = \"https://oss.sonatype.org/content/repositories/snapshots\" } } Maven <dependency> <groupId>com.github.housepower</groupId> <artifactId>clickhouse-spark-runtime-3.3_2.12</artifactId> <version>${version}</version> </dependency> Add the following repository if you want to use SNAPSHOT version. <repositories> <repository> <id>sonatype-oss-snapshots</id> <name>Sonatype OSS Snapshots Repository</name> <url>https://oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories>","title":"Get the Library"},{"location":"quick_start/01_get_the_library/#get-the-library","text":"","title":"Get the Library"},{"location":"quick_start/01_get_the_library/#download-the-library","text":"The name pattern of binary jar is clickhouse-spark-runtime-${spark_binary_version}_${scala_binary_version}-${version}.jar , you can find all available released jars under Maven Central Repository and all daily build snapshot jars under Sonatype OSS Snapshots Repository . Tip The runtime jar contains and relocates all required classes, you do not need to worry about transitive dependencies management and potential class conflictions.","title":"Download the Library"},{"location":"quick_start/01_get_the_library/#import-as-dependency","text":"","title":"Import as Dependency"},{"location":"quick_start/01_get_the_library/#gradle","text":"dependencies { implementation(\"com.github.housepower:clickhouse-spark-runtime-3.3_2.12:${version}\") } Add the following repository if you want to use SNAPSHOT version. repositries { maven { url = \"https://oss.sonatype.org/content/repositories/snapshots\" } }","title":"Gradle"},{"location":"quick_start/01_get_the_library/#maven","text":"<dependency> <groupId>com.github.housepower</groupId> <artifactId>clickhouse-spark-runtime-3.3_2.12</artifactId> <version>${version}</version> </dependency> Add the following repository if you want to use SNAPSHOT version. <repositories> <repository> <id>sonatype-oss-snapshots</id> <name>Sonatype OSS Snapshots Repository</name> <url>https://oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories>","title":"Maven"},{"location":"quick_start/02_play_with_spark_sql/","text":"Play with Spark SQL Note: For SQL-only use cases, Apache Kyuubi(Incubating) is recommended for Production. Launch Spark SQL CLI $SPARK_HOME/bin/spark-sql \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:{version} to avoid copying jar to your Spark client node. Operations Basic operations, e.g. create database, create table, write table, read table, etc. spark-sql> use clickhouse; Time taken: 0.016 seconds spark-sql> create database if not exists test_db; Time taken: 0.022 seconds spark-sql> show databases; default system test_db Time taken: 0.289 seconds, Fetched 3 row(s) spark-sql> CREATE TABLE test_db.tbl_sql ( > create_time TIMESTAMP NOT NULL, > m INT NOT NULL COMMENT 'part key', > id BIGINT NOT NULL COMMENT 'sort key', > value STRING > ) USING ClickHouse > PARTITIONED BY (m) > TBLPROPERTIES ( > engine = 'MergeTree()', > order_by = 'id', > settings.index_granularity = 8192 > ); Time taken: 0.242 seconds spark-sql> insert into test_db.tbl_sql values > (timestamp'2021-01-01 10:10:10', 1, 1L, '1'), > (timestamp'2022-02-02 10:10:10', 2, 2L, '2') > as tabl(create_time, m, id, value); Time taken: 0.276 seconds spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 Time taken: 0.116 seconds, Fetched 2 row(s) spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 1.028 seconds spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 0.462 seconds spark-sql> select count(*) from test_db.tbl_sql; 6 Time taken: 1.421 seconds, Fetched 1 row(s) spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 Time taken: 0.123 seconds, Fetched 6 row(s)","title":"Play with Spark SQL"},{"location":"quick_start/02_play_with_spark_sql/#play-with-spark-sql","text":"Note: For SQL-only use cases, Apache Kyuubi(Incubating) is recommended for Production.","title":"Play with Spark SQL"},{"location":"quick_start/02_play_with_spark_sql/#launch-spark-sql-cli","text":"$SPARK_HOME/bin/spark-sql \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:{version} to avoid copying jar to your Spark client node.","title":"Launch Spark SQL CLI"},{"location":"quick_start/02_play_with_spark_sql/#operations","text":"Basic operations, e.g. create database, create table, write table, read table, etc. spark-sql> use clickhouse; Time taken: 0.016 seconds spark-sql> create database if not exists test_db; Time taken: 0.022 seconds spark-sql> show databases; default system test_db Time taken: 0.289 seconds, Fetched 3 row(s) spark-sql> CREATE TABLE test_db.tbl_sql ( > create_time TIMESTAMP NOT NULL, > m INT NOT NULL COMMENT 'part key', > id BIGINT NOT NULL COMMENT 'sort key', > value STRING > ) USING ClickHouse > PARTITIONED BY (m) > TBLPROPERTIES ( > engine = 'MergeTree()', > order_by = 'id', > settings.index_granularity = 8192 > ); Time taken: 0.242 seconds spark-sql> insert into test_db.tbl_sql values > (timestamp'2021-01-01 10:10:10', 1, 1L, '1'), > (timestamp'2022-02-02 10:10:10', 2, 2L, '2') > as tabl(create_time, m, id, value); Time taken: 0.276 seconds spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 Time taken: 0.116 seconds, Fetched 2 row(s) spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 1.028 seconds spark-sql> insert into test_db.tbl_sql select * from test_db.tbl_sql; Time taken: 0.462 seconds spark-sql> select count(*) from test_db.tbl_sql; 6 Time taken: 1.421 seconds, Fetched 1 row(s) spark-sql> select * from test_db.tbl_sql; 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2021-01-01 10:10:10 1 1 1 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 2022-02-02 10:10:10 2 2 2 Time taken: 0.123 seconds, Fetched 6 row(s)","title":"Operations"},{"location":"quick_start/03_play_with_spark_shell/","text":"Play with Spark Shell Launch Spark Shell $SPARK_HOME/bin/spark-shell \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:{version} to avoid copying jar to your Spark client node. Operations Basic operations, e.g. create database, create table, write table, read table, etc. scala> spark.sql(\"use clickhouse\") res0: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"create database test_db\") res1: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"show databases\").show +---------+ |namespace| +---------+ | default| | system| | test_db| +---------+ scala> spark.sql(\"\"\" | CREATE TABLE test_db.tbl ( | create_time TIMESTAMP NOT NULL, | m INT NOT NULL COMMENT 'part key', | id BIGINT NOT NULL COMMENT 'sort key', | value STRING | ) USING ClickHouse | PARTITIONED BY (m) | TBLPROPERTIES ( | engine = 'MergeTree()', | order_by = 'id', | settings.index_granularity = 8192 | ) | \"\"\") res2: org.apache.spark.sql.DataFrame = [] scala> :paste // Entering paste mode (ctrl-D to finish) spark.createDataFrame(Seq( (\"2021-01-01 10:10:10\", 1L, \"1\"), (\"2022-02-02 10:10:10\", 2L, \"2\") )).toDF(\"create_time\", \"id\", \"value\") .withColumn(\"create_time\", to_timestamp($\"create_time\")) .withColumn(\"m\", month($\"create_time\")) .select($\"create_time\", $\"m\", $\"id\", $\"value\") .writeTo(\"test_db.tbl\") .append // Exiting paste mode, now interpreting. scala> spark.table(\"test_db.tbl\").show +-------------------+---+---+-----+ | create_time| m| id|value| +-------------------+---+---+-----+ |2021-01-01 10:10:10| 1| 1| 1| |2022-02-02 10:10:10| 2| 2| 2| +-------------------+---+---+-----+","title":"Play with Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#play-with-spark-shell","text":"","title":"Play with Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#launch-spark-shell","text":"$SPARK_HOME/bin/spark-shell \\ --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\ --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\ --conf spark.sql.catalog.clickhouse.grpc_port=${CLICKHOUSE_GRPC_PORT:-9100} \\ --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\ --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\ --conf spark.sql.catalog.clickhouse.database=default \\ --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar The following argument --jars /path/clickhouse-spark-runtime-3.3_2.12-{version}.jar can be replaced by --repositories https://{maven-cental-mirror or private-nexus-repo} \\ --packages com.github.housepower:clickhouse-spark-runtime-3.3_2.12:{version} to avoid copying jar to your Spark client node.","title":"Launch Spark Shell"},{"location":"quick_start/03_play_with_spark_shell/#operations","text":"Basic operations, e.g. create database, create table, write table, read table, etc. scala> spark.sql(\"use clickhouse\") res0: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"create database test_db\") res1: org.apache.spark.sql.DataFrame = [] scala> spark.sql(\"show databases\").show +---------+ |namespace| +---------+ | default| | system| | test_db| +---------+ scala> spark.sql(\"\"\" | CREATE TABLE test_db.tbl ( | create_time TIMESTAMP NOT NULL, | m INT NOT NULL COMMENT 'part key', | id BIGINT NOT NULL COMMENT 'sort key', | value STRING | ) USING ClickHouse | PARTITIONED BY (m) | TBLPROPERTIES ( | engine = 'MergeTree()', | order_by = 'id', | settings.index_granularity = 8192 | ) | \"\"\") res2: org.apache.spark.sql.DataFrame = [] scala> :paste // Entering paste mode (ctrl-D to finish) spark.createDataFrame(Seq( (\"2021-01-01 10:10:10\", 1L, \"1\"), (\"2022-02-02 10:10:10\", 2L, \"2\") )).toDF(\"create_time\", \"id\", \"value\") .withColumn(\"create_time\", to_timestamp($\"create_time\")) .withColumn(\"m\", month($\"create_time\")) .select($\"create_time\", $\"m\", $\"id\", $\"value\") .writeTo(\"test_db.tbl\") .append // Exiting paste mode, now interpreting. scala> spark.table(\"test_db.tbl\").show +-------------------+---+---+-----+ | create_time| m| id|value| +-------------------+---+---+-----+ |2021-01-01 10:10:10| 1| 1| 1| |2022-02-02 10:10:10| 2| 2| 2| +-------------------+---+---+-----+","title":"Operations"}]}